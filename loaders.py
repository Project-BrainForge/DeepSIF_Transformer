from torch.utils.data import Dataset
import numpy as np
from scipy.io import loadmat, savemat
import os
import glob
from utils import add_white_noise, ispadding
import random
import torch


class LabeledDatasetLoader(Dataset):
    """
    Dataset loader for the pre-processed labeled dataset format
    
    Each .mat file (generated by extract_labeled_data.py) contains:
    - eeg_data: (500, 75) - Pre-processed EEG sensor data
    - source_data: (500, 994) - Pre-processed source space ground truth
    - labels: Active source region labels (already without padding)
    - index: Sample index
    - snr: Signal-to-noise ratio
    
    This loader simply loads the pre-processed data without additional processing.
    """

    def __init__(self, data_root, fwd=None, transform=None, args_params=None):
        self.data_root = data_root
        self.fwd = fwd  # Keep for compatibility, but data is already processed
        self.transform = transform
        
        # Get all .mat files in the directory
        if os.path.isdir(data_root):
            self.file_list = glob.glob(os.path.join(data_root, "sample_*.mat"))
            if not self.file_list:
                # Fallback to any .mat files if sample_*.mat not found
                self.file_list = glob.glob(os.path.join(data_root, "*.mat"))
        else:
            # Single file
            self.file_list = [data_root]
            
        self.file_list.sort()  # Ensure consistent ordering
        
        # Handle dataset length parameter
        if args_params and 'dataset_len' in args_params:
            self.dataset_len = min(args_params['dataset_len'], len(self.file_list))
        else:
            self.dataset_len = len(self.file_list)
            
        print(f"LabeledDatasetLoader initialized with {self.dataset_len} samples from {len(self.file_list)} files")
        
    def __len__(self):
        return self.dataset_len
        
    def __getitem__(self, index):
        # Load the pre-processed .mat file
        file_path = self.file_list[index]
        mat_data = loadmat(file_path)
        
        # Extract pre-processed data (no additional processing needed)
        eeg_data = mat_data['eeg_data'].astype('float32')  # (500, 75) - already processed
        source_data = mat_data['source_data'].astype('float32')  # (500, 994) - already processed
        labels = mat_data['labels']  # Active source region labels
        sample_index = mat_data['index']
        snr = mat_data['snr']
        
        # Handle different label formats
        if labels.ndim > 1:
            # If labels is 2D (from original dataset format), flatten and remove padding
            labels = labels.flatten()
            valid_labels = labels[labels != 15213]  # Remove padding values
        else:
            # If labels is already 1D (from extract_labeled_data.py), use directly
            valid_labels = labels[labels != 15213] if 15213 in labels else labels
            
        # Pad labels to consistent length for batching (max 70 as in original DeepSIF)
        padded_labels = np.full(70, 15213, dtype=np.int32)  # Initialize with padding value
        if len(valid_labels) > 0:
            padded_labels[:len(valid_labels)] = valid_labels.astype(np.int32)
            
        # Handle index and snr formats
        if hasattr(sample_index, 'flatten'):
            sample_index = sample_index.flatten()[0] if sample_index.size > 0 else index
        if hasattr(snr, 'flatten'):
            snr = snr.flatten()[0] if snr.size > 0 else 10.0
            
        # Create sample dictionary (data already processed by extract_labeled_data.py)
        sample = {
            'data': eeg_data,  # Pre-processed EEG data
            'nmm': source_data,  # Pre-processed source data
            'label': padded_labels,  # Padded source region labels for consistent batching
            'valid_labels': valid_labels.astype(np.int32).tolist(),  # Convert to list to avoid batching issues
            'snr': float(snr),  # SNR value
            'index': int(sample_index)  # Sample index
        }
        
        if self.transform:
            sample = self.transform(sample)
            
        return sample


class SpikeEEGBuild(Dataset):
    """Original DeepSIF dataset loader - kept for compatibility"""

    def __init__(self, data_root, fwd, transform=None, args_params=None):

        # args_params: optional parameters; can be dataset_len

        self.file_path = data_root
        self.fwd = fwd
        self.transform = transform

        self.data = []
        self.dataset_meta = loadmat(self.file_path)
        if args_params and 'dataset_len' in args_params:
            self.dataset_len = args_params['dataset_len']
        else:   # use the whole dataset
            self.dataset_len = self.dataset_meta['selected_region'].shape[0]
        if args_params and 'num_scale_ratio' in args_params:
            self.num_scale_ratio = args_params['num_scale_ratio']
        else:
            self.num_scale_ratio = self.dataset_meta['scale_ratio'].shape[2]

    def __getitem__(self, index):

        if not self.data:
            import h5py
            self.data = h5py.File('{}_nmm.h5'.format(self.file_path[:-12]), 'r')['data']

        raw_lb = self.dataset_meta['selected_region'][index].astype(np.int)         # labels with padding
        lb = raw_lb[np.logical_not(ispadding(raw_lb))]                              # labels without padding
        raw_nmm = np.zeros((500, self.fwd.shape[1]))

        for kk in range(raw_lb.shape[0]):                                           # iterate through number of sources
            curr_lb = raw_lb[kk, np.logical_not(ispadding(raw_lb[kk]))]
            current_nmm = self.data[self.dataset_meta['nmm_idx'][index][kk]]

            ssig = current_nmm[:, [curr_lb[0]]]                                     # waveform in the center region
            # set source space SNR
            ssig = ssig / np.max(ssig) * self.dataset_meta['scale_ratio'][index][kk][random.randint(0, self.num_scale_ratio - 1)]
            current_nmm[:, curr_lb] = ssig.reshape(-1, 1)
            # set weight decay inside one source patch
            weight_decay = self.dataset_meta['mag_change'][index][kk]
            weight_decay = weight_decay[np.logical_not(ispadding(weight_decay))]
            current_nmm[:, curr_lb] = ssig.reshape(-1, 1) * weight_decay

            raw_nmm = raw_nmm + current_nmm

        eeg = np.matmul(self.fwd, raw_nmm.transpose())                              # project data to sensor space; num_electrode * num_time
        csnr = self.dataset_meta['sensor_snr'][index]
        noisy_eeg = add_white_noise(eeg, csnr).transpose()

        noisy_eeg = noisy_eeg - np.mean(noisy_eeg, axis=0, keepdims=True)  # time
        noisy_eeg = noisy_eeg - np.mean(noisy_eeg, axis=1, keepdims=True)  # channel
        noisy_eeg = noisy_eeg / np.max(np.abs(noisy_eeg))

        # get the training output
        empty_nmm = np.zeros_like(raw_nmm)
        empty_nmm[:, lb] = raw_nmm[:, lb]
        empty_nmm = empty_nmm / np.max(empty_nmm)
        # Each data sample
        sample = {'data': noisy_eeg.astype('float32'),
                  'nmm': empty_nmm.astype('float32'),
                  'label': raw_lb,
                  'snr': csnr}
        if self.transform:
            sample = self.transform(sample)

        return sample

    def __len__(self):
        return self.dataset_len


class SpikeEEGLoad(Dataset):
    """Dataset, load pregenerated input/output pair"""

    def __init__(self, data_root, fwd, transform=None, args_params=None):
        self.file_path = data_root
        self.fwd = fwd
        self.transform = transform
        if args_params and 'dataset_len' in args_params:
            self.dataset_len = args_params['dataset_len']
        else:   # use the whole dataset
            self.dataset_len = len(glob.glob(os.path.join(data_root, 'data*.mat')))

    def __getitem__(self, index):
        # load data saved as separate files using loadmat
        raw_data = loadmat('{}/data{}'.format(self.file_path, index))
        sample = {'data': raw_data['data'].astype('float32'),
                  'nmm': raw_data['nmm'].astype('float32'),
                  'label': raw_data['label'],
                  'snr': raw_data['csnr']}

        if self.transform:
            sample = self.transform(sample)

        return sample

    def __len__(self):
        return self.dataset_len
